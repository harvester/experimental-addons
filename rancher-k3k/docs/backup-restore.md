# Rancher k3k Backup and Restore

Guide for backing up and restoring a Rancher management server running inside a k3k virtual cluster on Harvester.

## Why Backup/Restore?

Common scenarios:

- **PVC Resize**: k3k PVCs cannot be shrunk. To reduce storage (e.g. 500Gi to 20Gi), you must destroy and recreate the vcluster.
- **Storage Class Migration**: Moving from one Longhorn storage class to another.
- **Disaster Recovery**: Rebuilding after data loss or cluster failure.
- **Host Cluster Migration**: Moving Rancher to a different Harvester cluster.

## What Gets Backed Up

| Data | Backup Method | Restore Method |
| ---- | ------------- | -------------- |
| Deployment config (hostname, versions, PVC size) | Captured from live cluster | Applied to new cluster |
| HelmChart CRs (cert-manager, Rancher) | Exported from vcluster | Re-created from templates |
| TLS certificates | Exported from vcluster + host | Re-generated by cert-manager |
| Rancher settings | Exported via API | Manual reconfiguration |
| Imported clusters list | Exported via API | Manual re-import |
| API tokens (metadata) | Exported via API | Recreated via terraform-setup.sh |
| Host ingress config | Exported from host | Re-created from templates |
| Registry config secrets | Exported from host | Re-applied |
| Private CA cert | Exported from vcluster | Re-applied |

## What Must Be Recreated

These cannot be restored from backup and must be recreated:

- **Imported clusters**: Rancher generates new registration tokens on each deployment. Clusters must be re-imported via the Rancher UI.
- **API tokens**: New tokens must be created via `terraform-setup.sh`.
- **Cloud credentials**: Must be regenerated from the new Rancher instance.
- **TLS certificates**: cert-manager generates new self-signed certs (or requests new Let's Encrypt certs).
- **Rancher internal state**: User sessions, audit logs, fleet state.

## Prerequisites

- `kubectl` configured to access the Harvester host cluster
- `jq`, `curl`, `helm` installed
- Rancher admin credentials (for API state backup)

## Backup Procedure

### Quick Backup (Config Only)

Captures deployment configuration without Rancher API state. Use when Rancher is not reachable or you just need the config.

```bash
./backup.sh
# Press Enter when prompted for Rancher credentials to skip API backup
```

### Full Backup (Config + API State)

Captures deployment configuration and Rancher API state (settings, cluster list, token metadata).

```bash
./backup.sh
# Enter admin credentials when prompted
```

### Specify Output Directory

```bash
./backup.sh --output /path/to/my-backup
```

### Backup Contents

After running `backup.sh`, the backup directory contains:

```text
backups/20260215-143000/
├── config.json                      # Deployment configuration manifest
├── kubeconfig-k3k.yaml              # k3k virtual cluster access
├── k3k-cluster.yaml                 # k3k Cluster CR (full YAML)
├── helmcharts.yaml                  # HelmChart CRs from vcluster
├── tls-rancher-ingress-vcluster.yaml  # TLS cert from inside vcluster
├── tls-rancher-ingress-host.yaml      # TLS cert from host namespace
├── tls-ca.yaml                        # Private CA (if configured)
├── host-service.yaml                  # Host-side Service
├── host-ingress.yaml                  # Host-side Ingress
├── ingress-reconciler.yaml            # CronJob
├── ingress-watcher.yaml               # Deployment
├── k3s-registry-config.yaml           # Registry mirrors (if configured)
├── k3s-registry-ca.yaml               # Registry CA (if configured)
├── rancher-settings.json              # Rancher API settings
├── imported-clusters.json             # List of imported clusters
├── api-tokens.json                    # API token metadata
└── cloud-credentials.json             # Cloud credential metadata
```

## Restore Procedure

### Basic Restore (Same Config)

Restores with the exact same configuration as the backup:

```bash
./restore.sh --from ./backups/20260215-143000
```

### Restore with PVC Resize

Restores with a different PVC size (the primary use case):

```bash
./restore.sh --from ./backups/20260215-143000 --pvc-size 20Gi
```

### Dry Run

Preview what would be done without making changes:

```bash
./restore.sh --from ./backups/20260215-143000 --pvc-size 20Gi --dry-run
```

### Restore Options

| Flag | Description |
| ---- | ----------- |
| `--from <dir>` | Backup directory (required) |
| `--pvc-size <size>` | Override PVC size (e.g. `20Gi`) |
| `--dry-run` | Preview only, no changes |
| `--skip-terraform` | Don't prompt for terraform-setup.sh |
| `--k3k-repo <url>` | Override k3k Helm repo URL |

## PVC Resize Workflow

Complete workflow to resize the k3k vcluster PVC:

```bash
# 1. Backup current state
./backup.sh
# Enter admin credentials when prompted

# 2. Verify backup
cat backups/<timestamp>/config.json | jq .storage

# 3. Destroy existing vcluster
./destroy.sh
# Type "yes" to confirm

# 4. Restore with new PVC size
./restore.sh --from ./backups/<timestamp> --pvc-size 20Gi

# 5. Wait for Rancher to be accessible
curl -sk https://rancher.example.com/ping

# 6. Re-import Harvester
#    Rancher UI → Virtualization Management → Import Existing

# 7. Create new API token
./terraform-setup.sh

# 8. Update downstream Terraform configs with new token + cluster ID
```

## Post-Restore Checklist

After a successful restore, complete these steps:

- [ ] Rancher UI accessible at configured hostname
- [ ] Admin login works with bootstrap password
- [ ] Set server-url in Rancher settings (if not auto-detected)
- [ ] Re-import Harvester cluster via Virtualization Management
- [ ] Verify Harvester shows as Active in Rancher
- [ ] Run `terraform-setup.sh` to create API token
- [ ] Update `terraform.tfvars` in downstream projects with:
  - New `rancher_token`
  - New `harvester_cluster_id`
  - New `kubeconfig-harvester-cloud-cred.yaml` (if applicable)
- [ ] Verify Terraform can plan against Rancher: `terraform plan`
- [ ] Deploy/verify downstream RKE2 clusters

## Troubleshooting

### Restore fails at "Waiting for k3k cluster to be ready"

Check the k3k server pod:

```bash
kubectl get pods -n rancher-k3k
kubectl logs -n rancher-k3k -l cluster=rancher,role=server
```

Common causes:

- Storage class doesn't exist (check `kubectl get sc`)
- Insufficient cluster resources

### Rancher not accessible after restore

Check the ingress chain:

```bash
# Host ingress
kubectl get ingress -n rancher-k3k

# TLS secret exists
kubectl get secret tls-rancher-ingress -n rancher-k3k

# k3k server pod is running
kubectl get pods -n rancher-k3k -l cluster=rancher,role=server

# Rancher pod inside vcluster
K3K_KC=./kubeconfig-k3k.yaml
kubectl --kubeconfig=$K3K_KC --insecure-skip-tls-verify get pods -n cattle-system
```

### "Cannot connect to k3k cluster" during restore

The NodePort may have changed. Extract the new kubeconfig:

```bash
kubectl get secret k3k-rancher-kubeconfig -n rancher-k3k \
  -o jsonpath='{.data.kubeconfig\.yaml}' | base64 -d > /tmp/k3k-kc.yaml

NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
NODE_PORT=$(kubectl get svc k3k-rancher-service -n rancher-k3k \
  -o jsonpath='{.spec.ports[?(@.port==443)].nodePort}')

echo "Server: https://${NODE_IP}:${NODE_PORT}"
```

### Imported cluster shows "Unavailable" after re-import

The cattle-cluster-agent on the imported cluster may have stale credentials from the old Rancher. Re-apply the registration manifest:

1. In Rancher UI, click the cluster → Registration
2. Copy the `kubectl apply` command
3. Run it on the imported cluster

## Limitations

- **No state migration**: Rancher internal state (users, RBAC, fleet bundles) is not migrated. The restored Rancher is a fresh installation with the same external configuration.
- **Re-import required**: All downstream/imported clusters must be manually re-imported.
- **Token regeneration**: API tokens are per-Rancher-instance. All integrations (Terraform, CI/CD) must be updated with new tokens.
- **TLS cert change**: If using self-signed TLS (`tls_source: rancher`), the certificate will be regenerated. Clients that pinned the old cert must be updated.

## Backup Retention

Backups are stored locally in `./backups/`. Recommended retention:

- Keep the last 3 backups
- Delete older backups: `ls -dt backups/*/ | tail -n +4 | xargs rm -rf`
- Consider copying critical backups to external storage (e.g. Harvester VM, NFS)
