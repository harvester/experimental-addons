---
# CronJob that reconciles the k3k Rancher host resources.
#
# Handles two failure modes that k3k does not manage:
#
# 1. Host Ingress deletion — The nginx Ingress can be deleted during Harvester
#    upgrades, ingress-nginx reconciliation, or node drains. The CronJob
#    recreates it if missing.
#
# 2. Flannel crash recovery — When the k3k server pod is rescheduled to a
#    different node, flannel's network policy controller crashes due to stale
#    node IP in etcd. The CronJob detects CrashLoopBackOff, confirms the
#    flannel error, and deletes the pod to reset the backoff timer for faster
#    recovery. It also ensures --disable-network-policy is set in serverArgs.
#
# Usage:
#   # Replace __HOSTNAME__ with your Rancher FQDN, then apply:
#   sed 's|__HOSTNAME__|rancher.example.com|g' ingress-reconciler.yaml | kubectl apply -f -
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress-reconciler
  namespace: k3k-rancher
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ingress-reconciler
  namespace: k3k-rancher
rules:
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses"]
    verbs: ["get", "create", "patch"]
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "create", "patch"]
  - apiGroups: [""]
    resources: ["pods", "pods/log"]
    verbs: ["get", "list", "delete"]
  - apiGroups: ["k3k.io"]
    resources: ["clusters"]
    verbs: ["get", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ingress-reconciler
  namespace: k3k-rancher
subjects:
  - kind: ServiceAccount
    name: ingress-reconciler
    namespace: k3k-rancher
roleRef:
  kind: Role
  name: ingress-reconciler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ingress-reconciler
  namespace: k3k-rancher
spec:
  schedule: "*/5 * * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: ingress-reconciler
          restartPolicy: Never
          containers:
            - name: reconciler
              image: alpine/kubectl:1.34.2
              command:
                - sh
                - -c
                - |
                  echo "=== Ingress reconciliation ==="

                  # Ensure Service exists
                  kubectl get svc k3k-rancher-traefik -n k3k-rancher &>/dev/null || {
                    echo "Service missing, recreating..."
                    kubectl apply -f - <<'SVC'
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: k3k-rancher-traefik
                    namespace: k3k-rancher
                  spec:
                    type: ClusterIP
                    selector:
                      cluster: rancher
                      role: server
                    ports:
                      - name: http
                        port: 80
                        targetPort: 80
                      - name: https
                        port: 443
                        targetPort: 443
                  SVC
                  }

                  # Ensure Ingress exists
                  kubectl get ingress k3k-rancher-ingress -n k3k-rancher &>/dev/null || {
                    echo "Ingress missing, recreating..."
                    kubectl apply -f - <<'ING'
                  apiVersion: networking.k8s.io/v1
                  kind: Ingress
                  metadata:
                    name: k3k-rancher-ingress
                    namespace: k3k-rancher
                    annotations:
                      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
                  spec:
                    ingressClassName: nginx
                    tls:
                      - hosts:
                          - __HOSTNAME__
                        secretName: tls-rancher-ingress
                    rules:
                      - host: __HOSTNAME__
                        http:
                          paths:
                            - path: /
                              pathType: Prefix
                              backend:
                                service:
                                  name: k3k-rancher-traefik
                                  port:
                                    number: 443
                  ING
                  }

                  echo "=== Flannel crash recovery ==="

                  # Check if k3k server pod is in CrashLoopBackOff
                  POD_STATUS=$(kubectl get pod k3k-rancher-server-0 -n k3k-rancher -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null || echo "")

                  if [ "$POD_STATUS" = "CrashLoopBackOff" ]; then
                    echo "Pod in CrashLoopBackOff, checking logs..."

                    # Check if it's the flannel network policy controller error
                    LOGS=$(kubectl logs k3k-rancher-server-0 -n k3k-rancher --previous --tail=5 2>/dev/null || echo "")

                    if echo "$LOGS" | grep -q "unable to initialize network policy controller"; then
                      echo "Flannel network policy crash detected!"

                      # Ensure --disable-network-policy is in serverArgs
                      CURRENT_ARGS=$(kubectl get clusters.k3k.io rancher -n k3k-rancher -o jsonpath='{.spec.serverArgs}' 2>/dev/null || echo "")
                      if ! echo "$CURRENT_ARGS" | grep -q "disable-network-policy"; then
                        echo "Patching Cluster CR with --disable-network-policy..."
                        kubectl patch clusters.k3k.io rancher -n k3k-rancher --type=merge \
                          -p '{"spec":{"serverArgs":["--disable-network-policy"]}}'
                      fi

                      # Delete the pod to reset backoff timer and pick up new args
                      echo "Deleting pod to reset backoff and apply new args..."
                      kubectl delete pod k3k-rancher-server-0 -n k3k-rancher --grace-period=0 2>/dev/null || true

                    else
                      echo "CrashLoopBackOff is not flannel-related, skipping"
                    fi
                  else
                    echo "Pod is healthy (status: ${POD_STATUS:-Running})"
                  fi

                  echo "=== Reconciliation complete ==="
